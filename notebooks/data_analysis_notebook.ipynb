{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project-introduction",
   "metadata": {},
   "source": [
    "# Data Analyst Agent - Interactive Notebook\n",
    "\n",
    "This notebook provides an interactive environment for the Data Analyst Agent.\n",
    "You can upload files, process them, and ask intelligent questions using AI.\n",
    "\n",
    "## Features:\n",
    "- Multi-modal file processing (CSV, Excel, PDF, Images, Text)\n",
    "- AI-powered data analysis using Together AI's LLaMA model\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Conversation logging\n",
    "- Interactive visualizations\n",
    "\n",
    "## Requirements:\n",
    "- Python 3.8+\n",
    "- Together AI API key\n",
    "- Required packages (see requirements.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment and logging\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# Add src directory to path\n",
    "src_path = Path(\"../src\").resolve()\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# Import our modules\n",
    "from data_processor import DataProcessor\n",
    "from ai_agent import AIAgent\n",
    "from utils import setup_logging, validate_api_key\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "api-setup",
   "metadata": {},
   "source": [
    "## 2. API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from environment or input\n",
    "api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "if not api_key or not validate_api_key(api_key):\n",
    "    print(\"‚ö†Ô∏è API key not found or invalid in environment variables.\")\n",
    "    print(\"Please enter your Together AI API key:\")\n",
    "    api_key = input(\"API Key: \")\n",
    "    os.environ[\"TOGETHER_API_KEY\"] = api_key\n",
    "\n",
    "if validate_api_key(api_key):\n",
    "    print(\"‚úÖ API key validated successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Invalid API key. Please check and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialize-components",
   "metadata": {},
   "source": [
    "## 3. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "component-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize data processor\n",
    "    data_processor = DataProcessor()\n",
    "    print(\"‚úÖ Data processor initialized\")\n",
    "    \n",
    "    # Initialize AI agent\n",
    "    ai_agent = AIAgent(\n",
    "        api_key=api_key,\n",
    "        model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"‚úÖ AI agent initialized and connected\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing components: {str(e)}\")\n",
    "    logger.error(f\"Component initialization failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file-processing",
   "metadata": {},
   "source": [
    "## 4. File Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Load and process a file from the local filesystem\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ Processing file: {file_path}\")\n",
    "    \n",
    "    # Create a mock uploaded file object\n",
    "    class MockUploadedFile:\n",
    "        def __init__(self, file_path):\n",
    "            self.name = os.path.basename(file_path)\n",
    "            self.size = os.path.getsize(file_path)\n",
    "            self._file_path = file_path\n",
    "            self._content = None\n",
    "        \n",
    "        def read(self):\n",
    "            if self._content is None:\n",
    "                with open(self._file_path, 'rb') as f:\n",
    "                    self._content = f.read()\n",
    "            return self._content\n",
    "        \n",
    "        def seek(self, position):\n",
    "            # For simplicity, we'll reload the file content\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        mock_file = MockUploadedFile(file_path)\n",
    "        processed_data = data_processor.process_file(mock_file)\n",
    "        \n",
    "        print(f\"‚úÖ File processed successfully!\")\n",
    "        \n",
    "        # Display basic info\n",
    "        if isinstance(processed_data, pd.DataFrame):\n",
    "            print(f\"üìä Dataset shape: {processed_data.shape}\")\n",
    "            print(f\"üìã Columns: {list(processed_data.columns)}\")\n",
    "        elif isinstance(processed_data, str):\n",
    "            print(f\"üìù Text length: {len(processed_data)} characters\")\n",
    "        \n",
    "        return processed_data, {\n",
    "            'name': mock_file.name,\n",
    "            'type': file_path.split('.')[-1].upper(),\n",
    "            'size': mock_file.size\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing file: {str(e)}\")\n",
    "        logger.error(f\"File processing error: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def display_data_info(data, file_info=None):\n",
    "    \"\"\"\n",
    "    Display information about the processed data\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        print(\"\\nüìä Data Overview:\")\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "        print(f\"Memory usage: {data.memory_usage().sum() / 1024:.1f} KB\")\n",
    "        print(\"\\nColumn types:\")\n",
    "        print(data.dtypes)\n",
    "        \n",
    "        print(\"\\nüìã First 5 rows:\")\n",
    "        display(data.head())\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = data.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(\"\\n‚ö†Ô∏è Missing values:\")\n",
    "            print(missing[missing > 0])\n",
    "        \n",
    "        # Basic statistics for numeric columns\n",
    "        numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(\"\\nüìà Basic statistics:\")\n",
    "            display(data[numeric_cols].describe())\n",
    "    \n",
    "    elif isinstance(data, str):\n",
    "        print(f\"\\nüìù Text data: {len(data)} characters\")\n",
    "        print(\"\\nPreview:\")\n",
    "        print(data[:500] + \"...\" if len(data) > 500 else data)\n",
    "\n",
    "print(\"‚úÖ File processing functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data-section",
   "metadata": {},
   "source": [
    "## 5. Load Sample Data\n",
    "\n",
    "Let's load some sample data to demonstrate the capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available sample files\n",
    "sample_data_dir = Path(\"../data/samples\")\n",
    "sample_files = []\n",
    "\n",
    "if sample_data_dir.exists():\n",
    "    for file_path in sample_data_dir.glob(\"*\"):\n",
    "        if file_path.is_file():\n",
    "            sample_files.append(str(file_path))\n",
    "\n",
    "if sample_files:\n",
    "    print(\"üìÅ Available sample files:\")\n",
    "    for i, file_path in enumerate(sample_files):\n",
    "        print(f\"{i+1}. {os.path.basename(file_path)}\")\n",
    "else:\n",
    "    print(\"üìÅ No sample files found. You can add files to ../data/samples/\")\n",
    "    \n",
    "    # Create a sample dataset\n",
    "    print(\"\\nüìä Creating a sample dataset...\")\n",
    "    np.random.seed(42)\n",
    "    sample_df = pd.DataFrame({\n",
    "        'date': pd.date_range('2023-01-01', periods=100),\n",
    "        'sales': np.random.normal(1000, 200, 100),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "        'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "        'customer_satisfaction': np.random.uniform(3, 5, 100)\n",
    "    })\n",
    "    \n",
    "    # Save sample data\n",
    "    os.makedirs(sample_data_dir, exist_ok=True)\n",
    "    sample_file_path = sample_data_dir / \"sample_sales_data.csv\"\n",
    "    sample_df.to_csv(sample_file_path, index=False)\n",
    "    print(f\"‚úÖ Sample data created: {sample_file_path}\")\n",
    "    \n",
    "    # Update sample files list\n",
    "    sample_files = [str(sample_file_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-sample-file",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first sample file\n",
    "if sample_files:\n",
    "    selected_file = sample_files[0]\n",
    "    print(f\"üîÑ Processing: {os.path.basename(selected_file)}\")\n",
    "    \n",
    "    current_data, current_file_info = load_and_process_file(selected_file)\n",
    "    \n",
    "    if current_data is not None:\n",
    "        display_data_info(current_data, current_file_info)\n",
    "else:\n",
    "    print(\"‚ùå No files to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ai-analysis-section",
   "metadata": {},
   "source": [
    "## 6. AI-Powered Analysis\n",
    "\n",
    "Now let's use the AI agent to analyze our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ai-analysis-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ai_question(question: str, data=None, file_info=None):\n",
    "    \"\"\"\n",
    "    Ask the AI agent a question about the data\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        data = current_data\n",
    "    if file_info is None:\n",
    "        file_info = current_file_info\n",
    "    \n",
    "    if data is None:\n",
    "        print(\"‚ùå No data loaded. Please process a file first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"ü§î Analyzing: {question}\")\n",
    "        print(\"‚è≥ Please wait...\")\n",
    "        \n",
    "        response = ai_agent.analyze_data(data, question, file_info)\n",
    "        \n",
    "        print(\"\\nü§ñ AI Response:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(response)\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting AI response: {str(e)}\")\n",
    "        logger.error(f\"AI analysis error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_summary_report(data=None, file_info=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary report\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        data = current_data\n",
    "    if file_info is None:\n",
    "        file_info = current_file_info\n",
    "    \n",
    "    return ai_agent.generate_summary_report(data, file_info)\n",
    "\n",
    "def suggest_analysis_questions(data=None, file_info=None):\n",
    "    \"\"\"\n",
    "    Get AI suggestions for analysis questions\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        data = current_data\n",
    "    if file_info is None:\n",
    "        file_info = current_file_info\n",
    "    \n",
    "    return ai_agent.suggest_questions(data, file_info)\n",
    "\n",
    "print(\"‚úÖ AI analysis functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-ai-questions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example AI questions\n",
    "if 'current_data' in locals() and current_data is not None:\n",
    "    \n",
    "    # Generate suggested questions\n",
    "    print(\"üîç Getting suggested analysis questions...\")\n",
    "    suggestions = suggest_analysis_questions()\n",
    "    if suggestions:\n",
    "        print(\"\\nüí° Suggested Questions:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(suggestions)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please load data first before running AI analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive analysis - ask your own questions\n",
    "if 'current_data' in locals() and current_data is not None:\n",
    "    \n",
    "    # Example questions you can ask:\n",
    "    example_questions = [\n",
    "        \"What are the main trends in this dataset?\",\n",
    "        \"Which category performs the best?\",\n",
    "        \"Are there any notable patterns or anomalies?\",\n",
    "        \"What insights can help improve business performance?\",\n",
    "        \"How does customer satisfaction vary across regions?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üí≠ Example questions you can ask:\")\n",
    "    for i, q in enumerate(example_questions, 1):\n",
    "        print(f\"{i}. {q}\")\n",
    "    \n",
    "    print(\"\\nü§ñ Ask a question about your data:\")\n",
    "    user_question = input(\"Your question: \")\n",
    "    \n",
    "    if user_question.strip():\n",
    "        response = ask_ai_question(user_question)\n",
    "    else:\n",
    "        print(\"No question provided.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please load data first before running AI analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-section",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(data):\n",
    "    \"\"\"\n",
    "    Perform comprehensive EDA on the dataset\n",
    "    \"\"\"\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        print(\"‚ùå EDA can only be performed on structured data (CSV/Excel)\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Performing Exploratory Data Analysis...\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nDataset shape: {data.shape}\")\n",
    "    print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    print(data.dtypes.value_counts())\n",
    "    \n",
    "    # Missing values\n",
    "    missing = data.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Missing Values:\")\n",
    "        missing_pct = (missing / len(data)) * 100\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Count': missing[missing > 0],\n",
    "            'Percentage': missing_pct[missing > 0]\n",
    "        })\n",
    "        display(missing_df)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values found\")\n",
    "    \n",
    "    # Numerical columns analysis\n",
    "    numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Numerical Columns ({len(numeric_cols)}):\")\n",
    "        display(data[numeric_cols].describe())\n",
    "        \n",
    "        # Visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Numerical Data Analysis', fontsize=16)\n",
    "        \n",
    "        # Histograms\n",
    "        if len(numeric_cols) >= 1:\n",
    "            col = numeric_cols[0]\n",
    "            axes[0, 0].hist(data[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[0, 0].set_title(f'Distribution of {col}')\n",
    "            axes[0, 0].set_xlabel(col)\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        if len(numeric_cols) >= 2:\n",
    "            col = numeric_cols[1] if len(numeric_cols) > 1 else numeric_cols[0]\n",
    "            axes[0, 1].boxplot(data[col].dropna())\n",
    "            axes[0, 1].set_title(f'Box Plot of {col}')\n",
    "            axes[0, 1].set_ylabel(col)\n",
    "        \n",
    "        # Correlation heatmap\n",
    "        if len(numeric_cols) > 1:\n",
    "            corr = data[numeric_cols].corr()\n",
    "            im = axes[1, 0].imshow(corr, cmap='coolwarm', aspect='auto')\n",
    "            axes[1, 0].set_title('Correlation Heatmap')\n",
    "            axes[1, 0].set_xticks(range(len(corr.columns)))\n",
    "            axes[1, 0].set_yticks(range(len(corr.columns)))\n",
    "            axes[1, 0].set_xticklabels(corr.columns, rotation=45)\n",
    "            axes[1, 0].set_yticklabels(corr.columns)\n",
    "            \n",
    "            # Add correlation values\n",
    "            for i in range(len(corr.columns)):\n",
    "                for j in range(len(corr.columns)):\n",
    "                    axes[1, 0].text(j, i, f'{corr.iloc[i, j]:.2f}', \n",
    "                                   ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Scatter plot (if we have at least 2 numeric columns)\n",
    "        if len(numeric_cols) >= 2:\n",
    "            col1, col2 = numeric_cols[0], numeric_cols[1]\n",
    "            axes[1, 1].scatter(data[col1], data[col2], alpha=0.6)\n",
    "            axes[1, 1].set_xlabel(col1)\n",
    "            axes[1, 1].set_ylabel(col2)\n",
    "            axes[1, 1].set_title(f'{col1} vs {col2}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüìä Categorical Columns ({len(categorical_cols)}):\")\n",
    "        \n",
    "        for col in categorical_cols[:3]:  # Show first 3 categorical columns\n",
    "            print(f\"\\n{col} - Unique values: {data[col].nunique()}\")\n",
    "            value_counts = data[col].value_counts().head(10)\n",
    "            print(value_counts)\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Perform EDA on current data\n",
    "if 'current_data' in locals() and current_data is not None:\n",
    "    perform_eda(current_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please load data first before performing EDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-section",
   "metadata": {},
   "source": [
    "## 8. Export and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(analysis_history, filename=None):\n",
    "    \"\"\"\n",
    "    Save analysis results to a file\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    if not filename:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"analysis_results_{timestamp}.json\"\n",
    "    \n",
    "    # Create outputs directory\n",
    "    output_dir = Path(\"../data/outputs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    # Prepare data for export\n",
    "    export_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'file_info': current_file_info if 'current_file_info' in locals() else None,\n",
    "        'analysis_history': analysis_history,\n",
    "        'data_summary': data_processor.get_data_summary(current_data) if 'current_data' in locals() else None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Analysis results saved to: {filepath}\")\n",
    "        return str(filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_analysis_report():\n",
    "    \"\"\"\n",
    "    Create a comprehensive analysis report\n",
    "    \"\"\"\n",
    "    if 'current_data' not in locals() or current_data is None:\n",
    "        print(\"‚ùå No data loaded for report generation\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìù Generating comprehensive analysis report...\")\n",
    "    \n",
    "    # Generate comprehensive summary\n",
    "    summary = generate_summary_report()\n",
    "    \n",
    "    if summary:\n",
    "        print(\"\\nüìã Comprehensive Data Analysis Report\")\n",
    "        print(\"=\" * 80)\n",
    "        print(summary)\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Save the report\n",
    "        report_data = [{\n",
    "            'type': 'comprehensive_summary',\n",
    "            'question': 'Generate comprehensive analysis report',\n",
    "            'response': summary,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }]\n",
    "        \n",
    "        save_analysis_results(report_data, \"comprehensive_report.json\")\n",
    "\n",
    "print(\"‚úÖ Export functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "if 'current_data' in locals() and current_data is not None:\n",
    "    create_analysis_report()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please load data first before generating a report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the capabilities of the Data Analyst Agent:\n",
    "\n",
    "### What we accomplished:\n",
    "- ‚úÖ Loaded and processed data files\n",
    "- ‚úÖ Performed exploratory data analysis\n",
    "- ‚úÖ Used AI to generate insights and answer questions\n",
    "- ‚úÖ Created visualizations and reports\n",
    "- ‚úÖ Saved analysis results\n",
    "\n",
    "### Next Steps:\n",
    "1. **Upload your own data**: Place files in `../data/samples/` and process them\n",
    "2. **Ask specific questions**: Use the AI agent to get detailed insights\n",
    "3. **Explore visualizations**: Create custom plots for your data\n",
    "4. **Export results**: Save your analysis for future reference\n",
    "5. **Use the Streamlit app**: Try the web interface for a more interactive experience\n",
    "\n",
    "### Tips for better analysis:\n",
    "- Be specific in your questions to the AI\n",
    "- Combine AI insights with statistical analysis\n",
    "- Validate AI responses with domain knowledge\n",
    "- Use visualizations to verify trends and patterns\n",
    "\n",
    "Happy analyzing! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
